{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPDGR7ejxORJMtGoy/ZMa6y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cham0703/DU_AN_SPAM/blob/main/main_email_detechtion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code 1"
      ],
      "metadata": {
        "id": "TYDWi5yknxeg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AVfldbkpmTcg",
        "outputId": "48d0430a-0c7e-44bc-cb32-cda54aa6a5c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'D:/spam.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a035f54cf34e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mduong_dan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"D:/spam.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# T·ª± ƒë·ªông ph√°t hi·ªán encoding c·ªßa file ƒë·ªÉ tr√°nh l·ªói khi ƒë·ªçc (m√°y tui b·ªã l·ªói f8 n√™n tui ch·∫°y chu·∫©n ƒëo√°n cho ch·∫Øc )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mduong_dan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchardet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# ƒê·ªçc th·ª≠ 10.000 byte\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:/spam.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import chardet\n",
        "from wordcloud import WordCloud\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "# üßæ B∆Ø·ªöC 1: ƒê·ªçc file t·ª´ ·ªï ƒëƒ©a\n",
        "duong_dan = \"D:/spam.csv\"\n",
        "# T·ª± ƒë·ªông ph√°t hi·ªán encoding c·ªßa file ƒë·ªÉ tr√°nh l·ªói khi ƒë·ªçc (m√°y tui b·ªã l·ªói f8 n√™n tui ch·∫°y chu·∫©n ƒëo√°n cho ch·∫Øc )\n",
        "with open(duong_dan, \"rb\") as f:\n",
        "    result = chardet.detect(f.read(10000))  # ƒê·ªçc th·ª≠ 10.000 byte\n",
        "    encoding = result['encoding']\n",
        "    print(f\"ƒê√£ ƒëo√°n encoding: {encoding}\")\n",
        "# ƒê·ªçc d·ªØ li·ªáu v√† chu·∫©n h√≥a c·ªôt\n",
        "df = pd.read_csv(duong_dan, encoding=encoding)\n",
        "df = df[['v1', 'v2']]                    # L·∫•y 2 c·ªôt c·∫ßn thi·∫øt\n",
        "df.columns = ['label', 'message']       # ƒê·∫∑t t√™n c·ªôt r√µ r√†ng\n",
        "# üìä B∆Ø·ªöC 2: Kh√°m ph√° d·ªØ li·ªáu (EDA)\n",
        "# Th·ªëng k√™ s·ªë l∆∞·ª£ng ham/spam\n",
        "print(\"S·ªë l∆∞·ª£ng ham/spam:\")\n",
        "print(df['label'].value_counts())\n",
        "# V·∫Ω bi·ªÉu ƒë·ªì c·ªôt s·ªë l∆∞·ª£ng\n",
        "sns.countplot(data=df, x='label')\n",
        "plt.title(\"S·ªë l∆∞·ª£ng tin nh·∫Øn ham/spam\")\n",
        "plt.show()\n",
        "# T√≠nh ƒë·ªô d√†i tin nh·∫Øn\n",
        "df['message_length'] = df['message'].apply(len)\n",
        "# V·∫Ω bi·ªÉu ƒë·ªì ph√¢n b·ªë ƒë·ªô d√†i tin nh·∫Øn\n",
        "sns.histplot(data=df, x='message_length', hue='label', bins=50, kde=True)\n",
        "plt.title(\"ƒê·ªô d√†i tin nh·∫Øn theo nh√£n\")\n",
        "plt.show()\n",
        "# ‚òÅÔ∏è B∆Ø·ªöC 3: T·∫°o WordCloud\n",
        "spam_text = ' '.join(df[df['label'] == 'spam']['message'])\n",
        "ham_text = ' '.join(df[df['label'] == 'ham']['message'])\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(WordCloud(width=500, height=300, background_color='white').generate(spam_text))\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Spam WordCloud\")\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(WordCloud(width=500, height=300, background_color='white').generate(ham_text))\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Ham WordCloud\")\n",
        "plt.show()\n",
        "# üßπ B∆Ø·ªöC 4: L√†m s·∫°ch tin nh·∫Øn\n",
        "# ƒê∆∞·ªùng d·∫´n ƒë·∫øn file spam.csv c·ªßa b·∫°n\n",
        "duong_dan = \"D:/spam.csv\"\n",
        "# Ph√°t hi·ªán encoding ƒë·ªÉ tr√°nh l·ªói Unicode\n",
        "with open(duong_dan, \"rb\") as f:\n",
        "    encoding = chardet.detect(f.read())['encoding']\n",
        "# ƒê·ªçc d·ªØ li·ªáu v√† ƒë·ªïi t√™n c·ªôt\n",
        "df = pd.read_csv(duong_dan, encoding=encoding)[['v1', 'v2']]\n",
        "df.columns = ['label', 'message']\n",
        "# T·∫£i stopwords n·∫øu ch∆∞a c√≥\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "# H√†m l√†m s·∫°ch vƒÉn b·∫£n\n",
        "def clean_message(msg):\n",
        "    msg = msg.lower()                              # Chuy·ªÉn sang ch·ªØ th∆∞·ªùng\n",
        "    msg = re.sub(r'\\d+', '', msg)                 # Xo√° ch·ªØ s·ªë\n",
        "    msg = re.sub(r'[^\\w\\s]', '', msg)             # Xo√° d·∫•u c√¢u\n",
        "    words = msg.split()                           # T√°ch t·ª´\n",
        "    words = [w for w in words if w not in stop_words]  # B·ªè stop words\n",
        "    return ' '.join(words)\n",
        "# T·∫°o c·ªôt m·ªõi ch·ª©a tin nh·∫Øn ƒë√£ l√†m s·∫°ch\n",
        "df['cleaned_message'] = df['message'].apply(clean_message)\n",
        "# Ch·ªâ gi·ªØ l·∫°i hai c·ªôt y√™u c·∫ßu: nh√£n v√† tin nh·∫Øn ƒë√£ l√†m s·∫°ch\n",
        "df_output = df[['label', 'cleaned_message']]\n",
        "# Hi·ªÉn th·ªã 5 d√≤ng ƒë·∫ßu ƒë·ªÉ ki·ªÉm tra\n",
        "print(df_output.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code 2"
      ],
      "metadata": {
        "id": "yC_dI-Trn1QR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "duong_dan = \"D:/spam.csv\"\n",
        "\n",
        "# B∆Ø·ªöC 1: Chuy·ªÉn ƒë·ªïi nh√£n sang d·∫°ng s·ªë\n",
        "# Chuy·ªÉn 'ham' th√†nh 0, 'spam' th√†nh 1\n",
        "df['label_num'] = df['label'].map({'ham': 0, 'spam': 1})\n",
        "\n",
        "# In ra ƒë·ªÉ ki·ªÉm tra\n",
        "print(\"S·ªë l∆∞·ª£ng m·∫´u theo nh√£n s·ªë:\")\n",
        "print(df['label_num'].value_counts())\n",
        "\n",
        "# B∆Ø·ªöC 2: Chuy·ªÉn ƒë·ªïi tin nh·∫Øn th√†nh ƒë·∫∑c tr∆∞ng s·ªë\n",
        "# S·ª≠ d·ª•ng CountVectorizer (M√¥ h√¨nh Bag of Words)\n",
        "count_vectorizer = CountVectorizer(max_features=5000)  # Gi·ªõi h·∫°n 5000 t·ª´ ph·ªï bi·∫øn nh·∫•t\n",
        "X_count = count_vectorizer.fit_transform(df['cleaned_message'])\n",
        "\n",
        "# S·ª≠ d·ª•ng TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(df['cleaned_message'])\n",
        "\n",
        "# In ra k√≠ch th∆∞·ªõc c·ªßa d·ªØ li·ªáu sau khi chuy·ªÉn ƒë·ªïi\n",
        "print(f\"K√≠ch th∆∞·ªõc d·ªØ li·ªáu CountVectorizer: {X_count.shape}\")\n",
        "print(f\"K√≠ch th∆∞·ªõc d·ªØ li·ªáu TF-IDF: {X_tfidf.shape}\")\n",
        "\n",
        "# B∆Ø·ªöC 3: Chia d·ªØ li·ªáu th√†nh t·∫≠p hu·∫•n luy·ªán v√† t·∫≠p ki·ªÉm tra\n",
        "# S·ª≠ d·ª•ng c·∫£ hai lo·∫°i ƒë·∫∑c tr∆∞ng ƒë·ªÉ so s√°nh\n",
        "# S·ª≠ d·ª•ng CountVectorizer\n",
        "X_count_train, X_count_test, y_count_train, y_count_test = train_test_split(\n",
        "    X_count, df['label_num'], test_size=0.2, random_state=42, stratify=df['label_num']\n",
        ")\n",
        "\n",
        "# S·ª≠ d·ª•ng TF-IDF\n",
        "X_tfidf_train, X_tfidf_test, y_tfidf_train, y_tfidf_test = train_test_split(\n",
        "    X_tfidf, df['label_num'], test_size=0.2, random_state=42, stratify=df['label_num']\n",
        ")\n",
        "\n",
        "# B∆Ø·ªöC 4: L∆∞u t·ª´ v·ª±ng ƒë·ªÉ s·ª≠ d·ª•ng sau n√†y\n",
        "count_vocab = count_vectorizer.get_feature_names_out()\n",
        "tfidf_vocab = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# In ra th√¥ng tin v·ªÅ t·∫≠p hu·∫•n luy·ªán v√† ki·ªÉm tra\n",
        "print(\"\\nTh√¥ng tin v·ªÅ b·ªô d·ªØ li·ªáu ƒë√£ chia:\")\n",
        "print(f\"S·ªë l∆∞·ª£ng m·∫´u hu·∫•n luy·ªán: {X_count_train.shape[0]}\")\n",
        "print(f\"S·ªë l∆∞·ª£ng m·∫´u ki·ªÉm tra: {X_count_test.shape[0]}\")\n",
        "print(f\"T·ª∑ l·ªá spam trong t·∫≠p hu·∫•n luy·ªán: {y_count_train.mean()*100:.2f}%\")\n",
        "print(f\"T·ª∑ l·ªá spam trong t·∫≠p ki·ªÉm tra: {y_count_test.mean()*100:.2f}%\")\n",
        "\n",
        "\n",
        "output_summary = {\n",
        "    'CountVectorizer': {\n",
        "        'X_train shape': X_count_train.shape,\n",
        "        'X_test shape': X_count_test.shape,\n",
        "        'y_train shape': y_count_train.shape,\n",
        "        'y_test shape': y_count_test.shape,\n",
        "        'vocabulary size': len(count_vocab)\n",
        "    },\n",
        "    'TfidfVectorizer': {\n",
        "        'X_train shape': X_tfidf_train.shape,\n",
        "        'X_test shape': X_tfidf_test.shape,\n",
        "        'y_train shape': y_tfidf_train.shape,\n",
        "        'y_test shape': y_tfidf_test.shape,\n",
        "        'vocabulary size': len(tfidf_vocab)\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"\\nT·ªïng k·∫øt:\")\n",
        "for method, details in output_summary.items():\n",
        "    print(f\"\\n{method}:\")\n",
        "    for key, value in details.items():\n",
        "        print(f\"  - {key}: {value}\")"
      ],
      "metadata": {
        "id": "d4fT9RIOn8hD",
        "outputId": "d5fec8dc-eeeb-4da2-9f4b-797474906d3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-620f75d84e3c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# B∆Ø·ªöC 1: Chuy·ªÉn ƒë·ªïi nh√£n sang d·∫°ng s·ªë\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Chuy·ªÉn 'ham' th√†nh 0, 'spam' th√†nh 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label_num'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'ham'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'spam'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# In ra ƒë·ªÉ ki·ªÉm tra\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    }
  ]
}